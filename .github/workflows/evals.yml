name: Agent Evaluations

on:
  # Run manually or on schedule
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to test'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'

env:
  PYTHON_VERSION: "3.12"
  UV_VERSION: "0.5.0"

jobs:
  run-evals:
    name: Run Agent Evaluations
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment || 'staging' }}
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: ${{ env.UV_VERSION }}

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --group dev

      - name: Run agent evaluations
        env:
          EVAL_API_ENDPOINT: ${{ secrets.API_ENDPOINT }}
          EVAL_TIMEOUT: "60"
        run: |
          uv run pytest tests/evals \
            -v \
            --tb=short \
            -m eval \
            --junitxml=eval-results.xml

      - name: Upload eval results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: eval-results
          path: eval-results.xml

      - name: Publish eval results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: eval-results.xml
          check_name: Agent Evaluation Results
          comment_mode: off
